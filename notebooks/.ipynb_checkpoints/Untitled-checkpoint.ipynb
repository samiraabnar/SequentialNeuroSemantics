{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string \n",
    "import scipy.io\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial import distance\n",
    "import itertools\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "import string \n",
    "import scipy.io\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial import distance\n",
    "import itertools\n",
    "from sklearn import svm\n",
    "import statsmodels.api as sm # import statsmodels \n",
    "from scipy import signal\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.linear_model import *\n",
    "\n",
    "class Scan(object):\n",
    "    def __init__(self,activations,timestamp, step,prev_words=None,next_words=None,all_words=None,all_pos=None,all_speak_features=None,current_translated_words=None):\n",
    "        self.activations = activations\n",
    "        self.timestamp = timestamp\n",
    "        self.prev_words = prev_words\n",
    "        self.next_words = next_words\n",
    "        self.step = step\n",
    "        self.all_words = all_words\n",
    "        self.current_translated_words = current_translated_words\n",
    "        self.all_pos = all_pos\n",
    "        self.all_speak_features = all_speak_features\n",
    "        self.brain3d = None\n",
    "        \n",
    "        \n",
    "        \n",
    "subject_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(dists,e_dists):\n",
    "    nn_index = np.argmin(dists,axis=1)\n",
    "    accuracy_on_test = np.mean(nn_index == np.argmax(np.eye(dists.shape[0]),axis=1))\n",
    "\n",
    "\n",
    "    b_acc = []\n",
    "    e_b_acc = []\n",
    "    for i,j in itertools.combinations(np.arange(dists.shape[0]), 2):\n",
    "        right_match = dists[i,i] + dists[j,j]\n",
    "        wrong_match = dists[i,j] + dists[j,i]\n",
    "        b_acc.append(right_match < wrong_match)\n",
    "\n",
    "        e_right_match = e_dists[i,i] + e_dists[j,j]\n",
    "        e_wrong_match = e_dists[i,j] + e_dists[j,i]\n",
    "        e_b_acc.append(e_right_match < e_wrong_match)\n",
    "\n",
    "    #print(\"binary accuracy: \", np.mean(b_acc),\" \", np.mean(e_b_acc))\n",
    "    return np.mean(b_acc),np.mean(e_b_acc),b_acc,e_b_acc\n",
    "\n",
    "def train_model(X,y):\n",
    "    #X = sm.add_constant(X) ## let's add an intercept (beta_0) to our model\n",
    "    # Note the difference in argument order\n",
    "    #model = sm.OLS(y, X).fit() ## sm.OLS(output, input)\n",
    "    \n",
    "    model = LinearRegression().fit(x,y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def MRR(distances):\n",
    "    prec_at_corrects = []\n",
    "    ranks = []\n",
    "    sorted_indexes = np.argsort(distances,axis=1)\n",
    "    for i in np.arange(len(distances)):\n",
    "        #print(i)\n",
    "        correct_at = np.where(sorted_indexes[i] == i)[0] + 1\n",
    "        #print(\"Reciprocal Rank\",correct_at)\n",
    "        prec_at_correct = 1.0/correct_at\n",
    "        #print(\"precision at \",correct_at,\": \",prec_at_correct)\n",
    "        prec_at_corrects.append(prec_at_correct)\n",
    "        ranks.append(correct_at)\n",
    "    \n",
    "    print(\"MRR: \",np.mean(prec_at_corrects),\" \",np.mean(ranks))\n",
    "    return np.mean(ranks), np.mean(prec_at_corrects), ranks,prec_at_corrects\n",
    "\n",
    "def test_model(model,X_t,y_t):\n",
    "    #X_t = sm.add_constant(X_t) ## let's add an intercept (beta_0) to our model\n",
    "    pred_t = model.predict(X_t)\n",
    "    \n",
    "    \n",
    "    argmax_accuracy = np.mean(np.argmax(pred_t, axis=1) == np.argmax(y_t,axis=1))\n",
    "    print(\"check argmax dim:\",np.argmax(pred_t, axis=1).shape)\n",
    "    print(\"argmax accuracy:\",argmax_accuracy)\n",
    "    cosine_dists = distance.cdist(pred_t,y_t,'cosine')\n",
    "    euc_dists =  distance.cdist(pred_t,y_t,'euclidean')\n",
    "    \n",
    "    print(\"cosine dist >>\")\n",
    "    mean_ranks_c = MRR(cosine_dists)\n",
    "    \n",
    "    print(\"euc_dists dist >>\")\n",
    "    mean_ranks_e = MRR(euc_dists)\n",
    "    \n",
    "    print(\"binary accuracy >>\")\n",
    "    c_acc, e_acc, _,_ = eval(cosine_dists,euc_dists)\n",
    "    print(c_acc,e_acc)\n",
    "    \n",
    "    return c_acc, e_acc, argmax_accuracy\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_brain_features(block_features, block_scans, block_ids):\n",
    "    brains = []\n",
    "    features = []\n",
    "    for block_id in block_ids:\n",
    "        for scan in block_scans[block_id]:\n",
    "            block_step = scan.step  - block_scans[block_id][0].step\n",
    "            brains.append(scan.activations[0])\n",
    "\n",
    "            related_features = []\n",
    "            i = 0\n",
    "            while i<4 and (block_step + i) < len(block_features[block_id]):\n",
    "                related_features.append(block_features[block_id][block_step + i])\n",
    "                i += 1\n",
    "\n",
    "            features.append(max(related_features))  \n",
    "    \n",
    "    print(len(features),len(brains))\n",
    "    brains = np.asarray(brains)\n",
    "    brains = (brains - np.min(brains,axis=0))/(np.max(brains,axis=0) - np.min(brains,axis=0) + 0.000001)\n",
    "    return brains, features     \n",
    "\n",
    "\n",
    "def evaluate_fold_brain2features(feature_model, block_scans,test_blocks,train_blocks):\n",
    "    brains_train, features_train = fold_brain_features(feature_model,block_scans,block_ids=train_blocks)\n",
    "    brains_test, features_test = fold_brain_features(feature_model,block_scans,block_ids=test_blocks)\n",
    "\n",
    "\n",
    "    clf = SVC(kernel='linear',class_weight='balanced', C=0.1, probability=True)\n",
    "    clf.fit(brains_train, features_train)\n",
    "\n",
    "    prediction_train = clf.predict(brains_train)\n",
    "    prediction_test = clf.predict(brains_test)\n",
    "\n",
    "    train_accuracy = np.mean(features_train == prediction_train)\n",
    "    test_accuracy = np.mean(features_test == prediction_test)\n",
    "\n",
    "    print(\"train accuracy: \",train_accuracy, \" test accuracy: \",test_accuracy)\n",
    "    cnf_matrix = confusion_matrix(features_test, prediction_test)\n",
    "    print(cnf_matrix)\n",
    "    \n",
    "    \n",
    "    result = []\n",
    "    accu_result = []\n",
    "    for i in np.arange(len(brains_test)):\n",
    "        if features_test[i] > 0:\n",
    "            j = np.where (np.argsort(prediction_test[i]) == np.asarray(features_test[i]))\n",
    "            if len(j.shape) > 1:\n",
    "                result.append(1/((np.max(features_test)+1) - j[0][0]))\n",
    "                accu_result.append(j[0][0] == 0)\n",
    "            else:\n",
    "                result.append(1/((np.max(features_test)+1) - j[0]))\n",
    "                accu_result.append(j[0] == 0)\n",
    "    print(\"MRR: \",np.mean(result), \"important accuracy:\",np.mean(accu_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_brain_vectors = np.load(\"../processed_data/512reducted_brain_scans.npy\")\n",
    "not_reduced_brain_vectors = np.load(\"../processed_data/reducted_brain_scans.npy\")\n",
    "reduced_brain_vectors_words = np.load(\"../processed_data/reducted_words.npy\")\n",
    "\n",
    "story_features = scipy.io.loadmat('../data/story_features.mat') \n",
    "subject_file = scipy.io.loadmat('../data/'+'subject_'+str(subject_id)+'.mat') \n",
    "part_of_speaches_feature_id = 8\n",
    "speach_feature_id = 1\n",
    "part_of_speaches = story_features['features'][0][part_of_speaches_feature_id][1][0]\n",
    "part_of_speaches_features = story_features['features'][0][part_of_speaches_feature_id][2]\n",
    "speaches = story_features['features'][0][speach_feature_id][1][0]\n",
    "speach_features = story_features['features'][0][speach_feature_id][2]\n",
    "\n",
    "\n",
    "speach_feature_id = 1\n",
    "motion_feature_id = 2\n",
    "emotion_feature_id = 3\n",
    "verbs_feature_id = 4\n",
    "characters_feature_id = 5\n",
    "visual_wordlength_feature_id = 6\n",
    "Word_Num_feature_id = 7\n",
    "part_of_speaches_feature_id = 8\n",
    "Dependency_role_feature_id = 9\n",
    "\n",
    "word_index = 0\n",
    "time_index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: array([680], dtype=uint16), 2: array([1384], dtype=uint16), 3: array([1942], dtype=uint16), 4: array([2702], dtype=uint16)}\n",
      "{1: array([339]), 2: array([691]), 3: array([970]), 4: array([1350])}\n"
     ]
    }
   ],
   "source": [
    "actual_words = []\n",
    "word_times = []\n",
    "word_pos = []\n",
    "speach_states = []\n",
    "for i in np.arange(subject_file['words'].shape[1]):\n",
    "    actual_words.append(subject_file['words'][0][i][word_index][0][0][0].strip().replace(\"@\",\"\"))\n",
    "    word_times.append(subject_file['words'][0][i][time_index][0][0])\n",
    "    word_pos.append(part_of_speaches_features[i])\n",
    "    speach_states.append(speach_features[i])\n",
    "    \n",
    "    \n",
    "blocks = subject_file['time'][:,1]\n",
    "scan_times = subject_file['time'][:,0]\n",
    "\n",
    "block_ends = {}\n",
    "block_ends_indexes = {}\n",
    "\n",
    "\n",
    "for block_id in np.arange(1,5):\n",
    "    block_ends_indexes[block_id] = np.where(scan_times == np.max(scan_times[np.where(blocks == block_id )]))[0]\n",
    "    block_ends[block_id] = scan_times[block_ends_indexes[block_id]]+2\n",
    "\n",
    "\n",
    "print(block_ends)\n",
    "print(block_ends_indexes)\n",
    "\n",
    "block_texts = {1:[],2:[],3:[],4:[]}\n",
    "block_pos = {1:[],2:[],3:[],4:[]}\n",
    "block_speach_state = {1:[],2:[],3:[],4:[]}\n",
    "block_steps = {1:[],2:[],3:[],4:[]}\n",
    "character_feature = {1:[],2:[],3:[],4:[]}\n",
    "block_id = 1\n",
    "for index in np.arange(len(actual_words)):\n",
    "    if word_times[index] > block_ends[block_id]:\n",
    "        block_id += 1\n",
    "    block_texts[block_id].append(str(actual_words[index].encode(\"ascii\",'ignore').decode()))\n",
    "    block_pos[block_id].append(word_pos[index])\n",
    "    block_speach_state[block_id].append(speach_states[index][0])\n",
    "    block_steps[block_id].append(index)\n",
    "    character_feature[block_id].append(story_features['features'][0][characters_feature_id][2][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "end_of_sentence_indexes = {1:[],2:[],3:[],4:[]}\n",
    "start_of_sentence_indexes = {1:[],2:[],3:[],4:[]}\n",
    "qout_indexes = {1:[],2:[],3:[],4:[]}\n",
    "inside_qout_indexes = {1:[],2:[],3:[],4:[]}\n",
    "\n",
    "for block_id in [1,2,3,4]:\n",
    "    start_of_sentence_indexes[block_id].append(0)\n",
    "    inside_qout = False\n",
    "    for i in np.arange(len(block_texts[block_id])):\n",
    "        \n",
    "        already_in = False   \n",
    "        if \"\\\"\" in block_texts[block_id][i]:\n",
    "            qout_indexes[block_id].append(i)\n",
    "            \n",
    "            checked = False\n",
    "            if block_texts[block_id][i].strip().startswith(\"\\\"\"):\n",
    "                if inside_qout is False :\n",
    "                    if already_in == False:\n",
    "                        inside_qout_indexes[block_id].append(i)\n",
    "                        already_in = True\n",
    "                inside_qout = not inside_qout\n",
    "                checked = True\n",
    "                \n",
    "            if block_texts[block_id][i].strip().endswith(\"\\\"\"):\n",
    "                if inside_qout is True :\n",
    "                    if already_in == False:\n",
    "                        inside_qout_indexes[block_id].append(i) \n",
    "                        already_in = True\n",
    "                inside_qout = not inside_qout\n",
    "                checked = True\n",
    "                \n",
    "            if checked == False:\n",
    "                print(block_texts[block_id][i])\n",
    "                \n",
    "        if inside_qout:\n",
    "            if already_in == False:\n",
    "                inside_qout_indexes[block_id].append(i)\n",
    "        #end/start of sentence\n",
    "        truth_table = [punc in block_texts[block_id][i] for punc in [\"!\",\".\",\"?\",\":\"]]\n",
    "        if True in truth_table:\n",
    "            end_of_sentence_indexes[block_id].append(i)\n",
    "            if i+1 < len(block_texts[block_id]):\n",
    "                start_of_sentence_indexes[block_id].append(i+1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inside_qout_feature = {1:[],2:[],3:[],4:[]}\n",
    "start_sentence_feature = {1:[],2:[],3:[],4:[]}\n",
    "end_sentence_feature = {1:[],2:[],3:[],4:[]}\n",
    "for block_id in [1,2,3,4]:\n",
    "    inside_qout_feature[block_id] = np.zeros((len(block_texts[block_id])))\n",
    "    inside_qout_feature[block_id][inside_qout_indexes[block_id]] = 1\n",
    "    \n",
    "    start_sentence_feature[block_id] = np.zeros((len(block_texts[block_id])))\n",
    "    start_sentence_feature[block_id][start_of_sentence_indexes[block_id]] = 1\n",
    "\n",
    "    end_sentence_feature[block_id] = np.zeros((len(block_texts[block_id])))\n",
    "    end_sentence_feature[block_id][end_of_sentence_indexes[block_id]] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brain_mapped_feature(block_feature):\n",
    "    brain_mapped_features = []\n",
    "    for block_id in [1,2,3,4]:\n",
    "        for j in np.arange(0,len(block_feature[block_id]),4):\n",
    "            i = 0\n",
    "            related_features = []\n",
    "            while i < 4 and (i+j) < len(block_feature[block_id]):\n",
    "                if max(block_feature[block_id][i+j]) == 0:\n",
    "                    related_features.append(0)\n",
    "                else:\n",
    "                    related_features.append(1+ np.argmax(block_feature[block_id][i+j]))\n",
    "                i += 1\n",
    "\n",
    "            brain_mapped_features.append(np.max(related_features))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block id 1\n",
      "block id 2\n",
      "block id 3\n",
      "block id 4\n"
     ]
    }
   ],
   "source": [
    "LSTM_embeddings_H_0 = {1:[],2:[],3:[],4:[]}\n",
    "LSTM_embeddings_H_1 = {1:[],2:[],3:[],4:[]}\n",
    "LSTM_embeddings_H_01 = {1:[],2:[],3:[],4:[]}\n",
    "LSTM_embeddings_M_0 = {1:[],2:[],3:[],4:[]}\n",
    "LSTM_embeddings_M_1 = {1:[],2:[],3:[],4:[]}\n",
    "LSTM_embeddings_M_01 = {1:[],2:[],3:[],4:[]}\n",
    "\n",
    "for block_id in [1,2,3,4]:    \n",
    "    print(\"block id\",block_id)\n",
    "    layer_id = 0\n",
    "    lstm_h_0 = np.load(\"../../lm1b/text_input_full_state/block_\"+str(block_id)+\"/lstm_hidden_\"+str(layer_id)+\".npy\")\n",
    "    lstm_c_0 = np.load(\"../../lm1b/text_input_full_state/block_\"+str(block_id)+\"/lstm_memory_\"+str(layer_id)+\".npy\")\n",
    "    layer_id = 1\n",
    "    lstm_h_1 = np.load(\"../../lm1b/text_input_full_state/block_\"+str(block_id)+\"/lstm_hidden_\"+str(layer_id)+\".npy\")\n",
    "    lstm_c_1 = np.load(\"../../lm1b/text_input_full_state/block_\"+str(block_id)+\"/lstm_memory_\"+str(layer_id)+\".npy\")\n",
    "  \n",
    "    for step in np.arange(len(block_texts[block_id])):\n",
    "        LSTM_embeddings_H_0[block_id].append(lstm_h_0.item()[step][0])\n",
    "        LSTM_embeddings_H_1[block_id].append(lstm_h_1.item()[step][0])\n",
    "        LSTM_embeddings_H_01[block_id].append(np.concatenate([lstm_h_0.item()[step][0],lstm_h_1.item()[step][0]]))\n",
    "        \n",
    "        LSTM_embeddings_M_0[block_id].append(lstm_c_0.item()[step][0])\n",
    "        LSTM_embeddings_M_1[block_id].append(lstm_c_1.item()[step][0])\n",
    "        LSTM_embeddings_M_01[block_id].append(np.concatenate([lstm_c_0.item()[step][0],lstm_c_1.item()[step][0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1303\n",
      "1303\n",
      "1303\n",
      "1303\n",
      "1303\n",
      "1303\n",
      "1303\n"
     ]
    }
   ],
   "source": [
    "print(len(inside_qout_feature[1]))\n",
    "print(len(start_sentence_feature[1]))\n",
    "print(len(end_sentence_feature[1]))\n",
    "print(len(block_pos[1]))\n",
    "print(len(block_speach_state[1]))\n",
    "print(len(character_feature[1]))\n",
    "print(len(LSTM_embeddings_H_0[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_scans = np.load(\"../processed_data/subject_\"+str(subject_id)+\"/subject_\"+str(subject_id)+\"_scan_objects.npy\").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1304\n"
     ]
    }
   ],
   "source": [
    "print(len(block_scans[1])*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(train_block_ids,test_block_ids,block_scans,block_features, mode=\"concat\"):\n",
    "    train_brain_activations = []\n",
    "    \n",
    "    train_features = {}\n",
    "    for feautre_key in block_features.keys(): \n",
    "        train_features[feautre_key] = []\n",
    "        \n",
    "    for train_block_id in train_block_ids:\n",
    "        for i in np.arange(len(block_scans[train_block_id])):\n",
    "            scan = block_scans[train_block_id][i]\n",
    "            train_brain_activations.append(scan.activations[0])\n",
    "        \n",
    "            step = scan.step - block_scans[train_block_id][0].step\n",
    "\n",
    "            for feautre_key in block_features.keys(): \n",
    "                i = 0\n",
    "                feature_values = []\n",
    "                \n",
    "                while i< 4 and (step+i) < len(block_features[feautre_key][train_block_id]):\n",
    "                    feature_values.append(block_features[feautre_key][train_block_id][step+i])\n",
    "                    i += 1\n",
    "                while  len(feature_values) < 4:\n",
    "                    feature_values.append(np.zeros_like(feature_values[-1]))\n",
    "                \n",
    "\n",
    "                if mode == \"concat\":\n",
    "                    if len(np.asarray(feature_values).shape) > 1:\n",
    "                        train_features[feautre_key].append(np.concatenate(feature_values))\n",
    "                    else:\n",
    "                        train_features[feautre_key].append(feature_values)\n",
    "\n",
    "                else:\n",
    "                    train_features[feautre_key].append(np.mean(feature_values,axis=0))\n",
    "                    \n",
    "       \n",
    "    test_brain_activations = []\n",
    "    test_features = {}\n",
    "    for feautre_key in block_features.keys(): \n",
    "        test_features[feautre_key] = []\n",
    "        \n",
    "    for test_block_id in test_block_ids:\n",
    "        for i in np.arange(len(block_scans[test_block_id])):\n",
    "            scan = block_scans[test_block_id][i]\n",
    "            test_brain_activations.append(scan.activations[0])\n",
    "        \n",
    "            step = scan.step - block_scans[test_block_id][0].step\n",
    "            for feautre_key in block_features.keys(): \n",
    "                i = 0\n",
    "                feature_values = []\n",
    "                while i< 4 and (step+i) < len(block_features[feautre_key][test_block_id]):\n",
    "                    feature_values.append(block_features[feautre_key][test_block_id][step+i])\n",
    "                    i += 1\n",
    "\n",
    "                while  len(feature_values) < 4:\n",
    "                    feature_values.append(np.zeros_like(feature_values[-1]))\n",
    "                    \n",
    "                if mode == \"concat\":\n",
    "                    if len(np.asarray(feature_values).shape) > 1:\n",
    "                        test_features[feautre_key].append(np.concatenate(feature_values))\n",
    "                    else:\n",
    "                        test_features[feautre_key].append(feature_values)\n",
    "                else:\n",
    "                    test_features[feautre_key].append(np.mean(feature_values,axis=0))\n",
    "\n",
    "    \n",
    "    return train_features,train_brain_activations,test_features,test_brain_activations\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1303\n",
      "1303\n",
      "1303\n",
      "1303\n",
      "1303\n",
      "1303\n",
      "1303\n"
     ]
    }
   ],
   "source": [
    "print(len(inside_qout_feature[1]))\n",
    "print(len(start_sentence_feature[1]))\n",
    "print(len(end_sentence_feature[1]))\n",
    "print(len(block_pos[1]))\n",
    "print(len(block_speach_state[1]))\n",
    "print(len(character_feature[1]))\n",
    "print(len(LSTM_embeddings_H_0[1]))\n",
    "\n",
    "block_features = {'inside_qout_feature':inside_qout_feature,\n",
    "                  'start_sentence_feature':start_sentence_feature,\n",
    "                  'end_sentence_feature':end_sentence_feature,\n",
    "                 'pos_tag':block_pos,\n",
    "                 'speach_state':block_speach_state,\n",
    "                 'character':character_feature,\n",
    "                 'LSTM_H_0':LSTM_embeddings_H_0,\n",
    "                 'LSTM_H_1':LSTM_embeddings_H_1,\n",
    "                 'LSTM_M_0':LSTM_embeddings_M_0,\n",
    "                 'LSTM_M_1':LSTM_embeddings_M_1,\n",
    "                 'LSTM_H_01':LSTM_embeddings_H_01,\n",
    "                 'LSTM_M_01':LSTM_embeddings_M_01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features,train_brain_activations,test_features,test_brain_activations = \\\n",
    "prepare_data([1,2,3],[4],block_scans,block_features, mode=\"concat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(929, 65536)\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(train_features['LSTM_M_01']).shape)\n",
    "X =  train_features['LSTM_M_01']## X usually means our input variables (or independent variables)\n",
    "y = np.asarray(train_brain_activations) ## Y usually means our output/dependent variable\n",
    "\n",
    "X_t = test_features['LSTM_M_01'] ## X usually means our input variables (or independent variables)\n",
    "y_t = np.asarray(test_brain_activations) ## Y usually means our output/dependent variable\n",
    "\n",
    "\n",
    "pca_x = PCA(n_components=512)\n",
    "pca_x.fit(X)\n",
    "\n",
    "X = pca_x.transform(X)\n",
    "X_t = pca_x.transform(X_t)\n",
    "\n",
    "pca_y = PCA(n_components=2048)\n",
    "pca_y.fit(y)\n",
    "\n",
    "y = pca_y.transform(y)\n",
    "y_t = pca_y.transform(y_t)\n",
    "#X = X - np.mean(X,axis=0)\n",
    "#X_t = X_t - np.mean(X_t, axis=0)\n",
    "#y = y - np.mean(y,axis=0)\n",
    "#y_t = y_t - np.mean(y_t, axis=0)\n",
    "\n",
    "#y = (y - np.min(y,axis=0))/ ( np.max(y,axis=0) - np.min(y,axis=0)+ 0.0000001)\n",
    "#y_t = (y_t - np.min(y_t,axis=0))/ ( np.max(y_t,axis=0) - np.min(y_t,axis=0)+ 0.0000001)\n",
    "\n",
    "X = (X - np.min(X,axis=0))/ ( np.max(X,axis=0) - np.min(X,axis=0)+ 0.00000001)\n",
    "X_t = (X_t - np.min(X_t,axis=0))/ ( np.max(X_t,axis=0) - np.min(X_t,axis=0)+ 0.000000001)\n",
    "\n",
    "model = train_model(X,y)\n",
    "test_model(model,X_t,y_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.asarray(train_features['LSTM_M_1']).shape)\n",
    "X =  train_features['LSTM_M_1']## X usually means our input variables (or independent variables)\n",
    "y = np.asarray(train_brain_activations) ## Y usually means our output/dependent variable\n",
    "\n",
    "X_t = test_features['LSTM_M_1'] ## X usually means our input variables (or independent variables)\n",
    "y_t = np.asarray(test_brain_activations) ## Y usually means our output/dependent variable\n",
    "\n",
    "\n",
    "#X = X - np.mean(X,axis=0)\n",
    "#X_t = X_t - np.mean(X_t, axis=0)\n",
    "#y = y - np.mean(y,axis=0)\n",
    "#y_t = y_t - np.mean(y_t, axis=0)\n",
    "\n",
    "y = (y - np.min(y,axis=0))/ ( np.max(y,axis=0) - np.min(y,axis=0)+ 0.0000001)\n",
    "y_t = (y_t - np.min(y_t,axis=0))/ ( np.max(y_t,axis=0) - np.min(y_t,axis=0)+ 0.0000001)\n",
    "\n",
    "X = (X - np.min(X,axis=0))/ ( np.max(X,axis=0) - np.min(X,axis=0)+ 0.00000001)\n",
    "X_t = (X_t - np.min(X_t,axis=0))/ ( np.max(X_t,axis=0) - np.min(X_t,axis=0)+ 0.000000001)\n",
    "\n",
    "model = train_model(X,y)\n",
    "test_model(model,X_t,y_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.asarray(train_features['LSTM_M_0']).shape)\n",
    "X =  train_features['LSTM_M_0']## X usually means our input variables (or independent variables)\n",
    "y = np.asarray(train_brain_activations) ## Y usually means our output/dependent variable\n",
    "\n",
    "X_t = test_features['LSTM_M_0'] ## X usually means our input variables (or independent variables)\n",
    "y_t = np.asarray(test_brain_activations) ## Y usually means our output/dependent variable\n",
    "\n",
    "\n",
    "#X = X - np.mean(X,axis=0)\n",
    "#X_t = X_t - np.mean(X_t, axis=0)\n",
    "#y = y - np.mean(y,axis=0)\n",
    "#y_t = y_t - np.mean(y_t, axis=0)\n",
    "\n",
    "#y = (y - np.min(y,axis=0))/ ( np.max(y,axis=0) - np.min(y,axis=0)+ 0.0000001)\n",
    "#y_t = (y_t - np.min(y_t,axis=0))/ ( np.max(y_t,axis=0) - np.min(y_t,axis=0)+ 0.0000001)\n",
    "\n",
    "X = (X - np.min(X,axis=0))/ ( np.max(X,axis=0) - np.min(X,axis=0)+ 0.00000001)\n",
    "X_t = (X_t - np.min(X_t,axis=0))/ ( np.max(X_t,axis=0) - np.min(X_t,axis=0)+ 0.000000001)\n",
    "\n",
    "model = train_model(X,y)\n",
    "test_model(model,X_t,y_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.asarray(train_features['LSTM_H_0']).shape)\n",
    "X =  train_features['LSTM_H_0']## X usually means our input variables (or independent variables)\n",
    "y = np.asarray(train_brain_activations) ## Y usually means our output/dependent variable\n",
    "\n",
    "X_t = test_features['LSTM_H_0'] ## X usually means our input variables (or independent variables)\n",
    "y_t = np.asarray(test_brain_activations) ## Y usually means our output/dependent variable\n",
    "\n",
    "\n",
    "#X = X - np.mean(X,axis=0)\n",
    "#X_t = X_t - np.mean(X_t, axis=0)\n",
    "#y = y - np.mean(y,axis=0)\n",
    "#y_t = y_t - np.mean(y_t, axis=0)\n",
    "\n",
    "#y = (y - np.min(y,axis=0))/ ( np.max(y,axis=0) - np.min(y,axis=0)+ 0.0000001)\n",
    "#y_t = (y_t - np.min(y_t,axis=0))/ ( np.max(y_t,axis=0) - np.min(y_t,axis=0)+ 0.0000001)\n",
    "\n",
    "X = (X - np.min(X,axis=0))/ ( np.max(X,axis=0) - np.min(X,axis=0)+ 0.00000001)\n",
    "X_t = (X_t - np.min(X_t,axis=0))/ ( np.max(X_t,axis=0) - np.min(X_t,axis=0)+ 0.000000001)\n",
    "\n",
    "model = train_model(X,y)\n",
    "test_model(model,X_t,y_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.asarray(train_features['LSTM_M_01']).shape)\n",
    "X =  train_features['LSTM_M_01']## X usually means our input variables (or independent variables)\n",
    "y = np.asarray(train_brain_activations) ## Y usually means our output/dependent variable\n",
    "\n",
    "X_t = test_features['LSTM_M_01'] ## X usually means our input variables (or independent variables)\n",
    "y_t = np.asarray(test_brain_activations) ## Y usually means our output/dependent variable\n",
    "\n",
    "\n",
    "#X = X - np.mean(X,axis=0)\n",
    "#X_t = X_t - np.mean(X_t, axis=0)\n",
    "#y = y - np.mean(y,axis=0)\n",
    "#y_t = y_t - np.mean(y_t, axis=0)\n",
    "\n",
    "#y = (y - np.min(y,axis=0))/ ( np.max(y,axis=0) - np.min(y,axis=0)+ 0.0000001)\n",
    "#y_t = (y_t - np.min(y_t,axis=0))/ ( np.max(y_t,axis=0) - np.min(y_t,axis=0)+ 0.0000001)\n",
    "\n",
    "X = (X - np.min(X,axis=0))/ ( np.max(X,axis=0) - np.min(X,axis=0)+ 0.00000001)\n",
    "X_t = (X_t - np.min(X_t,axis=0))/ ( np.max(X_t,axis=0) - np.min(X_t,axis=0)+ 0.000000001)\n",
    "\n",
    "model = train_model(X,y)\n",
    "test_model(model,X_t,y_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_features = np.concatenate([train_features['inside_qout_feature'],\n",
    "                              train_features['start_sentence_feature'],\n",
    "                              train_features['end_sentence_feature'],\n",
    "                              train_features['pos_tag'],\n",
    "                              train_features['speach_state'],\n",
    "                              train_features['character']],axis=0)\n",
    "print(the_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
