import tensorflow as tfimport numpy as npimport randomfrom collections import namedtupleimport numpy as npimport mathimport osimport globfrom HarryPotterDataProcessing import *from LinearMapper import LinearMapperfrom train import trainfrom eval import *tf.set_random_seed(1234)FLAGS = tf.app.flags.FLAGS# ========Where to save outputs===========tf.app.flags.DEFINE_string('log_root', '../log_root', 'Root directory for all logging.')tf.app.flags.DEFINE_string('data_path', '../data', 'Directory where the data '                                                   'is going to be saved.')tf.app.flags.DEFINE_string('exp_name', 'linear_map', 'Name for experiment. Logs will '                                                          'be saved in a directory with this'                                                          ' name, under log_root.')tf.app.flags.DEFINE_string('mapper', 'decoder', 'intended/forward/decoder')tf.app.flags.DEFINE_string('model', 'glove_linear', 'must be one of '                                               'char_word/word/contextual_0/contextual_1/contextual_01/char/glove/contextual_01_avg')tf.app.flags.DEFINE_string('direction', 'word2brain', 'must be one of '                                               'brain2word/word2brain')tf.app.flags.DEFINE_string('mode', 'eval_voxels', 'must be one of '                                               'train/test/save_vectors')tf.app.flags.DEFINE_string('weights', 'avg', 'must be one of '                                               'avg/trained')tf.app.flags.DEFINE_integer('linear_steps', '1', 'must be a positive integer')tf.app.flags.DEFINE_integer('select', '0', 'must be a positive integer')tf.app.flags.DEFINE_string('subject_id', '1', '1-8')tf.app.flags.DEFINE_integer('ith', '-1', 'which word to look at (-1 means all!')tf.app.flags.DEFINE_string('features', 'selected', 'dim_reducted/selected')# ==========Hyper Params=========tf.app.flags.DEFINE_integer('batch_size', 16, 'minibatch size')tf.app.flags.DEFINE_integer('hidden_dim', 256, 'dimension of hidden states')tf.app.flags.DEFINE_integer('input_dim', 784, 'size of the input')tf.app.flags.DEFINE_integer('output_dim', 784, 'size of the output')tf.app.flags.DEFINE_float('p_keep_input',.9,'positive float')tf.app.flags.DEFINE_float('p_keep_hidden',.6,'positive float')tf.app.flags.DEFINE_float('l2_factor',0.0,'positive float')# ===== Training Setup=======tf.app.flags.DEFINE_integer('number_of_epochs', 200, 'number_of_epochs')tf.app.flags.DEFINE_integer('training_size', 20, 'training_size')def prepare(FLAGS):    tf.logging.set_verbosity(tf.logging.INFO)  # choose what level of logging you want    # Change log_root to FLAGS.log_root/FLAGS.exp_name and create the dir if necessary    encoder_decoder_dir = os.path.join(FLAGS.log_root, 'subject_' + FLAGS.subject_id)    FLAGS.log_root = os.path.join(FLAGS.log_root, 'subject_' + FLAGS.subject_id, FLAGS.direction, FLAGS.model,                                  FLAGS.mapper, FLAGS.exp_name, )    if not os.path.exists(FLAGS.log_root):      os.makedirs(FLAGS.log_root)    test_embeddings, test_normalized_brain_scans, test_words, \    train_embeddings, train_normalized_brain_scans, train_size, train_words = load_data(FLAGS)    print("feature flag:", FLAGS.features)    if FLAGS.features == "dim_reducted":        with tf.Session() as sess:            # Restore the model from last checkpoints            dir = os.path.join(encoder_decoder_dir, "BrainAutoEncoder/detrended_standardized/best/")            saver = tf.train.import_meta_graph(dir + 'best_model_epoch19.meta')            saver.restore(sess, tf.train.latest_checkpoint(dir))            graph = tf.get_default_graph()            w_in = sess.run(graph.get_tensor_by_name("encoder_layer/w_in:0"))            w_out = sess.run(graph.get_tensor_by_name("decoder_layaer/w_out:0"))            b_in = sess.run(graph.get_tensor_by_name("encoder_layer/b_in:0"))            b_out = sess.run(graph.get_tensor_by_name("decoder_layaer/b_out:0"))            # Now, access the op that you want to run.            train_normalized_brain_scans = np.matmul(train_normalized_brain_scans, w_in) + b_in            test_normalized_brain_scans = np.matmul(test_normalized_brain_scans, w_in) + b_in            print("shape after reduction:", train_normalized_brain_scans.shape)    hps = compile_params(train_embeddings, train_normalized_brain_scans, train_size)    return hps, test_embeddings, test_normalized_brain_scans, test_words, train_embeddings, train_normalized_brain_scans, train_wordsdef compile_params(train_embeddings, train_normalized_brain_scans, train_size):    if FLAGS.direction == "word2brain":        FLAGS.input_dim = train_embeddings.shape[2]        FLAGS.output_dim = train_normalized_brain_scans.shape[1]    elif FLAGS.direction == "brain2word":        FLAGS.output_dim = train_embeddings.shape[2]        FLAGS.input_dim = train_normalized_brain_scans.shape[1]        print("input dim:",FLAGS.input_dim)    FLAGS.training_size = train_size    hparam_list = ['batch_size', 'hidden_dim', 'input_dim', 'output_dim', 'number_of_epochs', 'training_size', 'mode','linear_steps','l2_factor','weights']    hps_dict = {}    for key, val in FLAGS.__flags.items():  # for each flag        if key in hparam_list:  # if it's in the list            hps_dict[key] = val.value  # add it to the dict    hps = namedtuple("HParams", hps_dict.keys())(**hps_dict)    return hpsdef main(unused_argv):    hps, test_embeddings, test_normalized_brain_scans, test_words, \    train_embeddings, train_normalized_brain_scans, train_words = prepare(FLAGS)    print("test embeddings:",test_embeddings.shape)       print("train embeddings:",train_embeddings.shape)      with tf.Graph().as_default():        # Create a Supervisor that will checkpoint the model in '/tmp/mydir'.        train_dir = os.path.join(FLAGS.log_root, "train")        best_dir = os.path.join(FLAGS.log_root, "best")        if not os.path.exists(train_dir): os.makedirs(train_dir)        if not os.path.exists(best_dir): os.makedirs(best_dir)        mapper = LinearMapper(hps)        mapper.build_mapping_model()                best_saver = tf.train.Saver(max_to_keep=1)        if FLAGS.mode == "train":                        saver = tf.train.Saver(max_to_keep=3)                       sv = tf.train.Supervisor(logdir=train_dir,                                     is_chief=True,                                     saver=saver,                                     summary_op=None,                                     save_summaries_secs=60,  # save summaries for tensorboard every 60 secs                                     save_model_secs=60  # checkpoint every 60 secs                                     )            # Get a TensorFlow session managed by the supervisor.            with sv.managed_session() as sess:                if FLAGS.direction == "word2brain":                    if FLAGS.mode == "train":                        train(mapper, sess, sv, train_embeddings, train_normalized_brain_scans,                          test_embeddings, test_normalized_brain_scans,                          test_words=test_words,train_words=train_words,FLAGS=FLAGS,best_saver=best_saver,best_dir=best_dir)                    elif FLAGS.mode == "save_vectors":                        print("saving vectors...")                        save_pred_and_target_labesl(mapper, sess,test_embeddings,test_normalized_brain_scans,test_words,FLAGS,best_saver=best_saver,best_dir=best_dir)        elif FLAGS.mode == "eval_voxels":            with tf.Session() as sess:                print(best_dir)                meta_file = glob.glob(best_dir+"/*.meta")                print(meta_file)                ckpt = tf.train.get_checkpoint_state(best_dir)                best_saver.restore(sess, ckpt.model_checkpoint_path)                #saver = tf.train.import_meta_graph(meta_file[0])                #saver.restore(sess,tf.train.latest_checkpoint(best_dir))                #best_saver.recover_last_checkpoints(best_dir)                save_pred_and_target_labesl(mapper, sess,test_embeddings,test_normalized_brain_scans,test_words,FLAGS,"test")        elif FLAGS.mode == "eval_steps":            with tf.Session() as sess:                print("best dir: ",best_dir)                meta_file = glob.glob(best_dir+"/*.meta")                print(meta_file)                ckpt = tf.train.get_checkpoint_state(best_dir)                best_saver.restore(sess, ckpt.model_checkpoint_path)                print(sess.run(mapper.step_weights))if __name__ == '__main__':    tf.app.run()